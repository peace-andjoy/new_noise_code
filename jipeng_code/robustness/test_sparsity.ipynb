{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 45, 2)]           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               67072     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15)                1935      \n",
      "=================================================================\n",
      "Total params: 69,007\n",
      "Trainable params: 69,007\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import levy\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "def estimate_all_params_2(X, beta=None):\n",
    "    \n",
    "#     X = (X - X.mean())/X.std()\n",
    "\n",
    "    params = dict()\n",
    "#     params[\"mu\"], params['sigma'] = 0., 1.\n",
    "    if beta is not None: \n",
    "        params[\"beta\"] = beta\n",
    "    \n",
    "    params, neglog_density = levy.fit_levy(X)\n",
    "    p = params.__dict__\n",
    "    r = dict(zip(p[\"pnames\"], p[\"_x\"]))\n",
    "    r[\"log_density\"] = -neglog_density\n",
    "    # print(r)\n",
    "    return r\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def compute_sparsity(weight_values):\n",
    "    total_params = np.prod(weight_values.shape)  # 总参数数量\n",
    "    zero_params = np.sum(abs(weight_values) < 0.01)  # 零参数数量\n",
    "    # print(zero_params)\n",
    "    sparsity = zero_params / total_params  # 稀疏度\n",
    "\n",
    "    return sparsity\n",
    "\n",
    "for depth, width in [(3, 3)]:\n",
    "    print(f'depth={depth}, width={width}')\n",
    "    if depth != 3:\n",
    "        lr = 0.0005\n",
    "        iters = 50000\n",
    "    else:\n",
    "        lr = 0.001\n",
    "        iters = 10000\n",
    "    model = Sequential()\n",
    "    for j in range(depth-2):\n",
    "        model.add(Dense(width, activation='relu',input_shape=(1,784)))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# model = lstm_self(input_shape,nb_class,num_units,num_layers)\n",
    "model.load_weights('/home/xueqiongyuan/new_noise_code/models/libras_LSTM_width128_depth1_b32_alpha2.0_sigma0.2_num3_lr0.001_iter10000.h5')\n",
    "sparsity = []\n",
    "for layer in model.layers:\n",
    "    for weight in layer.weights:\n",
    "        # if 'bias' not in weight.name and 'batch_normalization' not in weight.name:\n",
    "        if 'dense' in weight.name:  \n",
    "            weight_values = weight.numpy()\n",
    "            # print(weight.name)\n",
    "            # plt.hist(weight_values.flatten(),bins = 50)\n",
    "            # plt.title(f\"Weight Histogram - {weight.name}\")\n",
    "            # plt.show()\n",
    "            # print(estimate_all_params_2(weight_values.flatten()[:10000]))\n",
    "            sparsity.append(compute_sparsity(weight_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11015625000000001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
    "from tensorflow.keras.layers import MaxPooling1D, Conv1D\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import BatchNormalization, GlobalAveragePooling1D, Permute, concatenate, Activation, add\n",
    "import numpy as np\n",
    "import math\n",
    "input_shape = (45, 2)\n",
    "nb_class = 15\n",
    "num_units = 128\n",
    "num_layers = 1\n",
    "def lstm_self(input_shape, nb_class, num_units, num_layers):\n",
    "    # Original proposal:\n",
    "    # S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” Neural Computation, vol. 9, no. 8, pp. 1735–1780, Nov. 1997.\n",
    "        \n",
    "    ip = Input(shape=input_shape)\n",
    "\n",
    "    if num_layers == 1:\n",
    "        l2 = LSTM(num_units)(ip)\n",
    "    elif num_layers == 2:\n",
    "        l1 = LSTM(num_units, return_sequences=True)(ip)\n",
    "        l2 = LSTM(num_units)(l1)\n",
    "    elif num_layers == 3:\n",
    "        l0 = LSTM(num_units, return_sequences=True)(ip)\n",
    "        l1 = LSTM(num_units, return_sequences=True)(l0)\n",
    "        l2 = LSTM(num_units)(l1)\n",
    "\n",
    "    out = Dense(nb_class, activation='softmax')(l2)\n",
    "\n",
    "    model = Model([ip], [out])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yxq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
